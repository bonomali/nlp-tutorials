{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_classification_tutorial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "9Dh01CILr1mZ",
        "WGdImz_Ur43x"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thinkingmachines/nlp-tutorials/blob/master/nlp_classification_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bqg0C6GwovuY",
        "colab_type": "text"
      },
      "source": [
        "# BERT for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dh01CILr1mZ",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkdopfjsZz0J",
        "colab_type": "code",
        "outputId": "3094f85d-60c8-48ca-905d-b8c4ea4c00b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!pip install tensorboardX\n",
        "!pip install pytorch-transformers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.16.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.0.1)\n",
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.185)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.82)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.6.8)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.185 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.185)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.185->boto3->pytorch-transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.185->boto3->pytorch-transformers) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.185->boto3->pytorch-transformers) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lhmcUMiTbC8",
        "colab_type": "code",
        "outputId": "2c8d0e60-7623-4d07-854b-a2266ebe7d88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Forked pytorch-transformers to make a slight edit to one of the DataProcessor classes\n",
        "!git clone https://github.com/thinkingmachines/pytorch-transformers.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pytorch-transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os2C-eUQYvPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Authenticate to GCS.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR4ch7awTbC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dl_gcs(object_fp, dest_fp, bucket_name='nlp-experiment-datasets', project_id='bert-experiments'):\n",
        "    from googleapiclient.discovery import build\n",
        "    gcs_service = build('storage', 'v1')\n",
        "    from apiclient.http import MediaIoBaseDownload\n",
        "    with open(dest_fp, 'wb') as f:\n",
        "        # Download the file from a given Google Cloud Storage bucket.\n",
        "        request = gcs_service.objects().get_media(bucket=bucket_name,\n",
        "                                            object=f'{object_fp}')\n",
        "        media = MediaIoBaseDownload(f, request)\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "    # _ is a placeholder for a progress object that we ignore.\n",
        "    # (Our file is small, so we skip reporting progress.)\n",
        "            _, done = media.next_chunk()        \n",
        "    print('Download complete')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pKhRL9rad5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mkdir cola"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bws2w8ITYmuy",
        "colab_type": "code",
        "outputId": "c33bcabc-06ef-4114-a659-82b06053d122",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dl_gcs('bert_toxic_class/train_10k.csv', 'cola/train.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lU4Rz71-ZAuh",
        "colab_type": "code",
        "outputId": "648daeb0-609e-4e89-be4b-87ec1264260d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dl_gcs('bert_toxic_class/test_10k.csv', 'cola/test.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-LioX9DItux_",
        "colab_type": "code",
        "outputId": "d1337172-2981-4706-afc9-e5c9632c5df8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dl_gcs('bert_toxic_class/dev_1k.csv', 'cola/dev.csv')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Download complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgOtEy5MY-ZN",
        "colab_type": "code",
        "outputId": "31550e66-1b81-4414-bf79-2d9a80421d50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mcola\u001b[0m/  \u001b[01;34mpytorch-transformers\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z37Qk7RfTbDD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy3FA6GyTbDF",
        "colab_type": "code",
        "outputId": "0f22565a-b63f-47fa-812c-0f80b9ce0c5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Setting chunksize for testing purposes\n",
        "train = pd.read_csv(f'cola/train.csv', engine='python')\n",
        "test = pd.read_csv(f'cola/test.csv', engine='python')\n",
        "dev = pd.read_csv(f'cola/dev.csv', engine='python')\n",
        "print(train.shape)\n",
        "# remove new lines etc.\n",
        "\n",
        "train['comment_text'] = train['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True).replace(r'\\t',  ' ', regex=True)\n",
        "test['comment_text'] = test['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True).replace(r'\\t',  ' ', regex=True)\n",
        "dev['comment_text'] = dev['comment_text'].replace({r'\\s+$': '', r'^\\s+': ''}, regex=True).replace(r'\\n',  ' ', regex=True).replace(r'\\t',  ' ', regex=True)\n",
        "\n",
        "# force train into cola format, test is fine as it is\n",
        "\n",
        "train['dummy_1'] = 'meh'\n",
        "train['dummy_2'] = '*'\n",
        "\n",
        "dev['dummy_1'] = 'meh'\n",
        "dev['dummy_2'] = '*'\n",
        "\n",
        "# Make sure that the target is on col index 1 and text is on col index 3\n",
        "train = train[['dummy_1','target','dummy_2','comment_text']]\n",
        "train['target'] = np.where(train['target']>=0.5,1,0)\n",
        "\n",
        "dev = dev[['dummy_1','target','dummy_2','comment_text']]\n",
        "dev['target'] = np.where(dev['target']>=0.5,1,0)\n",
        "\n",
        "# export as tab seperated\n",
        "\n",
        "#train = train[~train.index.isin([320, 321, 322, 323, 1190, 1191, 6412, 6413, 6414, 6415, 6416])]\n",
        "print(train.shape)\n",
        "train.to_csv('cola/train.tsv', sep='\\t', index=False, header=False)\n",
        "test.to_csv('cola/test.tsv', sep='\\t', index=False, header=True)\n",
        "dev.to_csv('cola/dev.tsv', sep='\\t', index=False, header=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 45)\n",
            "(10000, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FOxuhgYTbDJ",
        "colab_type": "code",
        "outputId": "f80e223b-529f-4283-88ae-e35d98b6d2e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Batch size originally at 8\n",
        "# take out --overwrite_output_dir if you don't want to oerwrite\n",
        "# add --do_eval to evaluate after training\n",
        "# We can set model_type to either bert or xlnet\n",
        "!python pytorch-transformers/examples/run_glue.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --task_name CoLA \\\n",
        "    --do_train \\\n",
        "    --model_type bert \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir cola \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=32   \\\n",
        "    --per_gpu_train_batch_size=32   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --output_dir /tmp/CoLA/ \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/17/2019 16:26:10 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "07/17/2019 16:26:11 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "07/17/2019 16:26:11 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "07/17/2019 16:26:12 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/17/2019 16:26:13 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "07/17/2019 16:26:17 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "07/17/2019 16:26:17 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "07/17/2019 16:26:20 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='cola', device=device(type='cuda'), do_eval=False, do_lower_case=True, do_train=True, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/tmp/CoLA/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=32, save_steps=50, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "07/17/2019 16:26:20 - INFO - __main__ -   Loading features from cached file cola/cached_train_bert-base-uncased_128_cola\n",
            "07/17/2019 16:26:20 - INFO - __main__ -   ***** Running training *****\n",
            "07/17/2019 16:26:20 - INFO - __main__ -     Num examples = 10000\n",
            "07/17/2019 16:26:20 - INFO - __main__ -     Num Epochs = 1\n",
            "07/17/2019 16:26:20 - INFO - __main__ -     Instantaneous batch size per GPU = 32\n",
            "07/17/2019 16:26:20 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "07/17/2019 16:26:20 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "07/17/2019 16:26:20 - INFO - __main__ -     Total optimization steps = 313\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/313 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/313 [00:01<08:10,  1.57s/it]\u001b[A\n",
            "Iteration:   1% 2/313 [00:03<07:56,  1.53s/it]\u001b[A\n",
            "Iteration:   1% 3/313 [00:04<07:43,  1.50s/it]\u001b[A\n",
            "Iteration:   1% 4/313 [00:05<07:33,  1.47s/it]\u001b[A\n",
            "Iteration:   2% 5/313 [00:07<07:26,  1.45s/it]\u001b[A\n",
            "Iteration:   2% 6/313 [00:08<07:21,  1.44s/it]\u001b[A\n",
            "Iteration:   2% 7/313 [00:10<07:17,  1.43s/it]\u001b[A\n",
            "Iteration:   3% 8/313 [00:11<07:14,  1.42s/it]\u001b[A\n",
            "Iteration:   3% 9/313 [00:12<07:11,  1.42s/it]\u001b[A\n",
            "Iteration:   3% 10/313 [00:14<07:09,  1.42s/it]\u001b[A\n",
            "Iteration:   4% 11/313 [00:15<07:08,  1.42s/it]\u001b[A\n",
            "Iteration:   4% 12/313 [00:17<07:06,  1.42s/it]\u001b[A\n",
            "Iteration:   4% 13/313 [00:18<07:04,  1.41s/it]\u001b[A\n",
            "Iteration:   4% 14/313 [00:19<07:02,  1.41s/it]\u001b[A\n",
            "Iteration:   5% 15/313 [00:21<07:02,  1.42s/it]\u001b[A\n",
            "Iteration:   5% 16/313 [00:22<07:00,  1.42s/it]\u001b[A\n",
            "Iteration:   5% 17/313 [00:24<06:59,  1.42s/it]\u001b[A\n",
            "Iteration:   6% 18/313 [00:25<06:58,  1.42s/it]\u001b[A\n",
            "Iteration:   6% 19/313 [00:27<06:57,  1.42s/it]\u001b[A\n",
            "Iteration:   6% 20/313 [00:28<06:56,  1.42s/it]\u001b[A\n",
            "Iteration:   7% 21/313 [00:29<06:54,  1.42s/it]\u001b[A\n",
            "Iteration:   7% 22/313 [00:31<06:53,  1.42s/it]\u001b[A\n",
            "Iteration:   7% 23/313 [00:32<06:51,  1.42s/it]\u001b[A\n",
            "Iteration:   8% 24/313 [00:34<06:50,  1.42s/it]\u001b[A\n",
            "Iteration:   8% 25/313 [00:35<06:49,  1.42s/it]\u001b[A\n",
            "Iteration:   8% 26/313 [00:36<06:47,  1.42s/it]\u001b[A\n",
            "Iteration:   9% 27/313 [00:38<06:46,  1.42s/it]\u001b[A\n",
            "Iteration:   9% 28/313 [00:39<06:45,  1.42s/it]\u001b[A\n",
            "Iteration:   9% 29/313 [00:41<06:44,  1.43s/it]\u001b[A\n",
            "Iteration:  10% 30/313 [00:42<06:43,  1.43s/it]\u001b[A\n",
            "Iteration:  10% 31/313 [00:44<06:41,  1.43s/it]\u001b[A\n",
            "Iteration:  10% 32/313 [00:45<06:40,  1.43s/it]\u001b[A\n",
            "Iteration:  11% 33/313 [00:46<06:39,  1.43s/it]\u001b[A\n",
            "Iteration:  11% 34/313 [00:48<06:37,  1.43s/it]\u001b[A\n",
            "Iteration:  11% 35/313 [00:49<06:35,  1.42s/it]\u001b[A\n",
            "Iteration:  12% 36/313 [00:51<06:33,  1.42s/it]\u001b[A\n",
            "Iteration:  12% 37/313 [00:52<06:32,  1.42s/it]\u001b[ATraceback (most recent call last):\n",
            "  File \"pytorch-transformers/examples/run_glue.py\", line 475, in <module>\n",
            "    main()\n",
            "  File \"pytorch-transformers/examples/run_glue.py\", line 429, in main\n",
            "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n",
            "  File \"pytorch-transformers/examples/run_glue.py\", line 132, in train\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/tensor.py\", line 107, in backward\n",
            "    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\", line 93, in backward\n",
            "    allow_unreachable=True)  # allow_unreachable flag\n",
            "KeyboardInterrupt\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGdImz_Ur43x",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yF7KMPP9uTvS",
        "colab_type": "code",
        "outputId": "e710eb84-656a-4415-ee6a-0be4c06b6977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Batch size originally at 8\n",
        "# Out of sample accuracy is at 44%, which is not so good, but this is expected given we only used 10k training examples out of 2M\n",
        "!python pytorch-transformers/examples/run_glue.py \\\n",
        "    --model_type bert \\\n",
        "    --model_name_or_path bert-base-uncased \\\n",
        "    --task_name CoLA \\\n",
        "    --do_eval \\\n",
        "    --do_lower_case \\\n",
        "    --data_dir cola \\\n",
        "    --max_seq_length 128 \\\n",
        "    --per_gpu_eval_batch_size=32   \\\n",
        "    --per_gpu_train_batch_size=32   \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --num_train_epochs 1.0 \\\n",
        "    --output_dir /tmp/CoLA/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "07/17/2019 16:18:43 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "07/17/2019 16:18:44 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n",
            "07/17/2019 16:18:44 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "07/17/2019 16:18:44 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
            "07/17/2019 16:18:45 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
            "07/17/2019 16:18:50 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
            "07/17/2019 16:18:50 - INFO - pytorch_transformers.modeling_utils -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "07/17/2019 16:18:52 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='', config_name='', data_dir='cola', device=device(type='cuda'), do_eval=True, do_lower_case=True, do_train=False, eval_all_checkpoints=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=2e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_seq_length=128, max_steps=-1, model_name_or_path='bert-base-uncased', model_type='bert', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/tmp/CoLA/', output_mode='classification', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=32, per_gpu_train_batch_size=32, save_steps=50, seed=42, server_ip='', server_port='', task_name='cola', tokenizer_name='', warmup_steps=0, weight_decay=0.0)\n",
            "07/17/2019 16:18:52 - INFO - __main__ -   Evaluate the following checkpoints: ['/tmp/CoLA/']\n",
            "07/17/2019 16:18:52 - INFO - pytorch_transformers.modeling_utils -   loading configuration file /tmp/CoLA/config.json\n",
            "07/17/2019 16:18:52 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "07/17/2019 16:18:52 - INFO - pytorch_transformers.modeling_utils -   loading weights file /tmp/CoLA/pytorch_model.bin\n",
            "07/17/2019 16:18:56 - INFO - __main__ -   Creating features from dataset file at cola\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   Writing example 0 of 1000\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   *** Example ***\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   guid: dev-0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   tokens: [CLS] it certainly is up to the casino and they don ' t want people of his il ##k in their casino . i wouldn ' t either . he ' s a cancer sc ##hem ##ing sc ##umb ##ag . [SEP]\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_ids: 101 2009 5121 2003 2039 2000 1996 9270 1998 2027 2123 1005 1056 2215 2111 1997 2010 6335 2243 1999 2037 9270 1012 1045 2876 1005 1056 2593 1012 2002 1005 1055 1037 4456 8040 29122 2075 8040 25438 8490 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   label: 1 (id = 1)\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   *** Example ***\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   guid: dev-1\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   tokens: [CLS] and are you familiar with the term , “ investigative reporting ? ” how about , the number four , as in four separate , independent women who had to be found by the reporters and who didn ’ t come forward of their own vol ##ition ? do you know the number 30 ? as in the four women ’ s stories were co ##rro ##bor ##ated by more than 30 other sources ? and innocence until proven guilty is a standard for a court proceeding - which can ’ t happen in these cases because the statute of limitations have long since expired on these crimes . [SEP]\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_ids: 101 1998 2024 2017 5220 2007 1996 2744 1010 1523 15025 7316 1029 1524 2129 2055 1010 1996 2193 2176 1010 2004 1999 2176 3584 1010 2981 2308 2040 2018 2000 2022 2179 2011 1996 12060 1998 2040 2134 1521 1056 2272 2830 1997 2037 2219 5285 22753 1029 2079 2017 2113 1996 2193 2382 1029 2004 1999 1996 2176 2308 1521 1055 3441 2020 2522 18933 12821 4383 2011 2062 2084 2382 2060 4216 1029 1998 12660 2127 10003 5905 2003 1037 3115 2005 1037 2457 18207 1011 2029 2064 1521 1056 4148 1999 2122 3572 2138 1996 11671 1997 12546 2031 2146 2144 13735 2006 2122 6997 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   label: 0 (id = 0)\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   *** Example ***\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   guid: dev-2\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   tokens: [CLS] fitting that you have changed your avatar to that of a comic nazi . read the sign : i know n ##nn ##nn ##nn ##not ##hing ! [SEP]\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_ids: 101 11414 2008 2017 2031 2904 2115 22128 2000 2008 1997 1037 5021 6394 1012 3191 1996 3696 1024 1045 2113 1050 10695 10695 10695 17048 12053 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   label: 0 (id = 0)\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   *** Example ***\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   guid: dev-3\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   tokens: [CLS] welcome to alaska ! [SEP]\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_ids: 101 6160 2000 7397 999 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   label: 0 (id = 0)\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   *** Example ***\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   guid: dev-4\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   tokens: [CLS] a trust ##fu ##nd millionaire pm rubbing elbows with trust ##fu ##nd billionaire ##s all the while telling small business owners that due to their un ##sop ##his ##tica ##tion , they really ought to be paying more taxes . [SEP]\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_ids: 101 1037 3404 11263 4859 19965 7610 10137 13690 2007 3404 11263 4859 22301 2015 2035 1996 2096 4129 2235 2449 5608 2008 2349 2000 2037 4895 28793 24158 22723 3508 1010 2027 2428 11276 2000 2022 7079 2062 7773 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   segment_ids: 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "07/17/2019 16:18:57 - INFO - utils_glue -   label: 0 (id = 0)\n",
            "07/17/2019 16:18:58 - INFO - __main__ -   Saving features into cached file cola/cached_dev_bert-base-uncased_128_cola\n",
            "07/17/2019 16:18:58 - INFO - __main__ -   ***** Running evaluation  *****\n",
            "07/17/2019 16:18:58 - INFO - __main__ -     Num examples = 1000\n",
            "07/17/2019 16:18:58 - INFO - __main__ -     Batch size = 32\n",
            "Evaluating: 100% 32/32 [00:14<00:00,  2.74it/s]\n",
            "07/17/2019 16:19:13 - INFO - __main__ -   ***** Eval results  *****\n",
            "07/17/2019 16:19:13 - INFO - __main__ -     mcc = 0.4419480431950892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve2HePGFkbn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}